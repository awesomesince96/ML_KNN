# -*- coding: utf-8 -*-
"""
Project Team:
    BINIT GAJERA(KH20736)
    KRISHNA PRAJAPATI(PI87410)

ML_Final_Project.ipynb

Automatically generated by Colaboratory.
"""

#If the code is to be executed on Google Colab then please use the below mentioned few lines
#Otherwise the code can directly be executed on a GPU supported machine.
#Please note that the dataset is quite huge, thus GPU should be able to handle such memory.

# =============================================================================
# # Install the PyDrive wrapper & import libraries.
# # This only needs to be done once per notebook.
# !pip install -U -q PyDrive
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
# 
# # Authenticate and create the PyDrive client.
# # This only needs to be done once per notebook.
# auth.authenticate_user()
# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
# 
# # Download a file based on its file ID.
# #
# # A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz
# file_id = 'file ID will be placed here'
# downloaded = drive.CreateFile({'id': file_id})
# downloaded.GetContentFile('MURA-v1.1-hand.zip')
# !unzip ./MURA-v1.1-hand.zip #File name to be unzipped
# #print('Downloaded content "{}"'.format(downloaded.GetContentString()))
# 
# # http://pytorch.org/
# from os.path import exists
# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'
# 
# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision
# import torch
# 
# print(torch.__version__)
# 
# #The below code needs to be executed in order to update the PIL library to the supported version
# # we need pillow version of 5.3.0
# # we will uninstall the older version first
# !pip uninstall -y Pillow
# # install the new one
# !pip install Pillow==5.3.0
# # import the new one
# import PIL
# print(PIL.PILLOW_VERSION)
# # this should print 5.3.0. If it doesn't, then restart your runtime:
# # Menu > Runtime > Restart Runtime
# 
# =============================================================================

#To ignore some unwanted warnings
import sys
if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

import numpy as np
#pandas is used to load the Dataset
import pandas as pd
import os

from PIL import Image

from tqdm import tqdm
import torch
import torch.functional as F
from torch.utils.data import DataLoader,Dataset
from torchvision.transforms import Normalize,CenterCrop,Resize,Compose

import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torchvision
from torchvision import datasets, models, transforms
from torchvision.datasets.folder import pil_loader
import matplotlib.pyplot as plt
import time
import copy
from torch.autograd import Variable

torch.cuda.is_available()

data_cat = ['train', 'valid'] # data categories

class Mura(Dataset):
    
    def __init__(self, df, transform=None):
      """
      df here would be containing the image path and labels.
      """
      self.df = df
      self.transform = transform

    def __len__(self):
      return len(self.df)

    def __getitem__(self, idx):
      study_p = self.df.iloc[idx, 0]
      count = self.df.iloc[idx, 1]
      images = []
      for i in range(count):
          image = pil_loader(study_p + 'image%s.png' % (i+1))
          images.append(self.transform(image))
      images = torch.stack(images)
      label = self.df.iloc[idx, 2]
      sample = {'images': images, 'label': label}
      return sample

mura_dataset = {}
study_label = {'positive': 1, 'negative': 0}
for phase in data_cat:
    BASE_DIR = 'MURA-v1.1-hand/%s/%s/' % (phase, 'XR_HAND')
    patients = list(os.walk(BASE_DIR))[0][1] #list of patient folder names
    mura_dataset[phase] = pd.DataFrame(columns=['Path', 'Count', 'Label'])
    i = 0
    for patient in tqdm(patients): #per patient folder
        for study in os.listdir(BASE_DIR + patient): # per study in that patient folder
            label = study_label[study.split('_')[1]] # get label 0 or 1
            path = BASE_DIR + patient + '/' + study + '/' # path to a study
            mura_dataset[phase].loc[i] = [path, len(os.listdir(path)), label] # add new row
            i+=1


# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(20),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) 
    ]),
    'valid': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

image_datasets = {x: Mura(mura_dataset[x], transform=data_transforms[x]) for x in data_cat}
dataloaders = {x: DataLoader(image_datasets[x], batch_size=1, shuffle=True, num_workers=4) for x in data_cat}

dataset_sizes = {x: len(mura_dataset[x]) for x in data_cat}

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def plot_data(data_loss, accs):
    train_acc = accs['train']
    valid_acc = accs['valid']
    train_loss = data_loss['train']
    valid_loss = data_loss['valid']
    epochs = range(len(train_acc))

    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1,)
    plt.plot(epochs, train_acc)
    plt.plot(epochs, valid_acc)
    plt.legend(['train', 'valid'], loc='upper left')
    plt.title('Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_loss)
    plt.plot(epochs, valid_loss)
    plt.legend(['train', 'valid'], loc='upper left')
    plt.title('Loss')

    plt.show()

def train_model(model, criterion, optimizer, dataloaders, scheduler, 
                dataset_sizes, num_epochs):
    since = time.time()
    #Keeping track of the best model with highest validation accuracy
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    data_loss = {x:[] for x in data_cat} # for storing loss per epoch
    accs = {x:[] for x in data_cat} # for storing accuracies per epoch
    print('Train batches:', len(dataloaders['train']))
    print('Valid batches:', len(dataloaders['valid']), '\n')
    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch+1, num_epochs))
        print('-' * 10)
        # Each epoch has a training and validation phase
        for phase in data_cat:
            model.train(phase=='train')
            running_loss = 0.0
            correct_preds = 0
            # Iterate over data.
            for i, data in enumerate(dataloaders[phase]):
                # get the inputs
                print(i, end='\r')
                inputs = data['images'][0]
                labels = data['label'].type(torch.FloatTensor)
                inputs = Variable(inputs.cuda())
                labels = Variable(labels.cuda())
                # zero the parameter gradients
                optimizer.zero_grad()
                # forward
                outputs = model(inputs)
                outputs = torch.mean(outputs)
                loss = criterion(outputs, labels, phase)
                running_loss += loss.data[0]
                # backward + optimize for training phase
                if phase == 'train':
                    loss.backward()
                    optimizer.step()
                # statistics
                preds = (outputs.data > 0.5).type(torch.cuda.FloatTensor)
                correct_preds += torch.sum(preds == labels.data)
            epoch_loss = running_loss.to(dtype=torch.float) / float(dataset_sizes[phase])
            epoch_acc = correct_preds.to(dtype=torch.float) / float(dataset_sizes[phase])
            data_loss[phase].append(epoch_loss)
            accs[phase].append(epoch_acc)
            print('{} Loss this epoch: {:.4f} Accuracy this epoch: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))
            # Keeping the copy of model with the best validation accuracy
            if phase == 'valid':
                scheduler.step(epoch_loss)
                if epoch_acc > best_acc:
                    best_acc = epoch_acc
                    best_model_wts = copy.deepcopy(model.state_dict())
        time_elapsed = time.time() - since
        print('Time elapsed: {:.0f}m {:.0f}s'.format(
                time_elapsed // 60, time_elapsed % 60))
        print()
    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best valid Acc: {:4f}'.format(best_acc))
    plot_data(data_loss, accs)
    # loading the best model after completing the training
    model.load_state_dict(best_model_wts)
    return model

def get_count(df, cat):
    '''
    df is the dataframe and cat contains "positive" for abnormal and "negative" for normal
    Returns number of images in a study type XR_HAND which are of abnormal or normal
    '''
    return df[df['Path'].str.contains(cat)]['Count'].sum()
  
def np_V(x):
    '''convert numpy float to Variable tensor float'''    
    return Variable(torch.cuda.FloatTensor([x]), requires_grad=False)

# ta = total abnormal images, tn = total normal images
ta = {x: get_count(mura_dataset[x], 'positive') for x in data_cat}
tn = {x: get_count(mura_dataset[x], 'negative') for x in data_cat}
W1 = {x: np_V(tn[x] / (tn[x] + ta[x])) for x in data_cat}
W0 = {x: np_V(ta[x] / (tn[x] + ta[x])) for x in data_cat}

class Loss(torch.nn.modules.Module):
    def __init__(self, W1, W0):
        super(Loss, self).__init__()
        self.W1 = W1
        self.W0 = W0
        
    def forward(self, inputs, targets, phase):
        loss = - (self.W1[phase] * targets * inputs.log() + self.W0[phase] * (1 - targets) * (1 - inputs).log())
        return loss

base_model=models.densenet169(pretrained=True)

for param in base_model.parameters():
    param.requires_grad = False

in_feat=base_model.classifier.out_features

#Adding a linear layer that produces only 1 class output and applying Sigmoid over it
model=nn.Sequential(
    base_model,
    nn.Linear(in_feat,1),
    nn.Sigmoid()
)

model=model.to(device)

criterion=Loss(W1, W0)
optimizer=optim.Adam(params=model.parameters(),lr=0.0001)
dynamic_lr_scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, verbose=True)

model = train_model(model, criterion, optimizer, dataloaders, dynamic_lr_scheduler, dataset_sizes, num_epochs=10)